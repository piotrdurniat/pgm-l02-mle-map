{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions as ptdist\n",
    "\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_values(\n",
    "    sample_sizes: List[int],\n",
    "    param_values: List[float],\n",
    "    true_param_value: float,\n",
    "    title: str,\n",
    "):\n",
    "    \"\"\"Plots changes of parameter value for different sampling sizes.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    x, y = sample_sizes, param_values\n",
    "    \n",
    "    ax.plot(\n",
    "        x, y, \n",
    "        marker='o', ms=3, linestyle='', \n",
    "        color='green', label='Estymator', \n",
    "    )\n",
    "    ax.plot(x, y, marker='', linestyle='--', alpha=0.2)\n",
    "\n",
    "    ax.axhline(\n",
    "        true_param_value, \n",
    "        linestyle='--', \n",
    "        color='blue',\n",
    "        label='Rzeczywista wartość parametru',\n",
    "    )\n",
    "    ax.set_xlabel('Rozmiar próbki')\n",
    "    ax.set_ylabel('Parametr')\n",
    "    ax.set_ylim((0, 1))\n",
    "    ax.legend()\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    \n",
    "def evaluate_at_intervals(\n",
    "    data: np.ndarray, \n",
    "    step: int, \n",
    "    fn: Callable,\n",
    "    start: int = 1,\n",
    ") -> Tuple[List[int], List[float]]:\n",
    "    sample_sizes = []\n",
    "    param_values = []\n",
    "    \n",
    "    for idx in range(start, len(data), step):\n",
    "        sample_sizes.append(idx)\n",
    "        param_values.append(fn(data, idx))\n",
    "        \n",
    "    return sample_sizes, param_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estymacja parametrów modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Źródła: [1](https://web.stanford.edu/class/archive/cs/cs109/cs109.1192/reader/11%20Parameter%20Estimation.pdf) [2](http://www.mi.fu-berlin.de/wiki/pub/ABI/Genomics12/MLvsMAP.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dotychczas mówiąc o rozkładach (modelach) zmiennych losowych mieliśmy podane ich parametry, np. zakładaliśmy, że $x \\sim N(0,1)$ - zmienna losowa $x$ pochodzi z rozkładu normalnego z parametrami średniej równej 0 oraz odchylenia standardowego równego 1 (rozkład normalny standardowy).\n",
    "\n",
    "W rzeczywistości rzadko kiedy będziemy mieli informację o rzeczywistych wartościach parametrów, a naszym zadaniem będzie przybliżyć (**wyestymować**) je na podstawie dostępnej **próbki danych**. Na tych zajęciach przeanalizujemy dwa podejścia do estymacji: **Maximum Likelihood Estimation (MLE)** oraz **Maximum a Posteriori (MAP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W poniższych rozważaniach będziemy oznaczać parametry modeli jako $\\theta$. Zakładamy, że zbiór danych składa się z $n$ próbek: $(X_1, X_2, \\ldots, X_n)$, które są niezależne i pochodzą z tego samego rozkładu prawdopodobieństwa (IID, *ang. independently and identically distributed*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykłady rozkładów oraz parametrów:\n",
    "\n",
    "| Nazwa           | Oznaczenie                | Parametry                    |\n",
    "| :-------------- | :------------------------ | :--------------------------- |\n",
    "| r. Bernoulliego | $$Bernoulli(p)$$          | $$\\theta = p$$               |\n",
    "| r. Poissona     | $$Poisson(\\lambda)$$      | $$\\theta = \\lambda$$         |\n",
    "| r. jednostajny  | $$Uniform(a, b)$$         | $$\\theta = (a, b)$$          |\n",
    "| r. normalny     | $$Normal(\\mu, \\sigma^2)$$ | $$\\theta = (\\mu, \\sigma^2)$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwaga: Istnieją różnice w polskiej i angielskiej terminologii nazywania rozkładów prawdopodobieństwa. \n",
    "\n",
    "W tej liście zadań będziemy korzystać z **rozkładu Bernoulliego**, zdefiniowanego tak jak w anglojęzycznych źródłach. Polskim odpowiednikiem jest \"**rozkład zero-jedynkowy**\". W celu zachowania  zgodności z nazwami występującymi w dostępnych bibliotekach programistycznych **będziemy się posługiwać angielską wersją (czyt: rozkład Bernoulliego)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku tej metody estymacji chcemy uzyskać parametry $\\theta$, przy których **obserwowane dane są najbardziej prawdopodobne**. \n",
    "\n",
    "Oznaczmy **likelihood** naszego rozkładu, niezależnie czy mówimy o rozkładach dyskretnych (funkcja masy prawdopodobieństwa) czy o rozkładach ciągłych (funkcja gęstości prawdopodobieństwa), jako $f(X|\\theta)$. Funkcja ta będzie nam określała jak prawdopodobne jest obserwowanie danych $X$ pod warunkiem parametrów $\\theta$.\n",
    "\n",
    "Czym się zatem różni likelihood od zwykłego prawdopodobieństwa? W przypadku rozkładów dyskretnych jest ono odpowiednikiem / synonimem łącznego prawdopodobieństwa, a w przypadku rozkładów ciągłych - łącznej gęstości prawdopodobieństwa.\n",
    "\n",
    "Założenie o IID próbki danych pozwala nam rozpisać łączny likelihood jako:\n",
    "$$L(X|\\theta) = \\prod_{i=1}^{n} f(X_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W metodzie MLE chcemy wybrać takie parametry $\\theta$, które zmaksymalizują $L(\\theta)$ - estymację tych parametrów oznaczymy jako $\\theta_\\text{MLE}$:\n",
    "\n",
    "$$\\theta_\\text{MLE} = \\text{argmax}_\\theta L(X|\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Często jednak nie będziemy odnosić się bezpośrednio do likelihood'u, tylko do logarytmu tej funkcji (tzw. **Log Likelihood**). Logarytm może być tutaj stosowany, ponieważ jest funkcją monotoniczną, a jego zastosowanie ułatwi kolejne obliczenia.\n",
    "\n",
    "$$ LL(X|\\theta) = log L(X|\\theta) = log \\prod_{i=1}^{n} f(X_i|\\theta) = \\sum_{i=1}^{n} log f(X_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE dla rozkładu Bernoulliego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że dane pochodzą z rozkładu Bernoulliego, tzn. $X_i \\sim \\text{Bernoulli}(p)$ dla próbki $n$ punktów danych $(X_1, X_2, \\ldots, X_n)$, które spełniają IID. Celem jest wyestymować wartość parametru $p$. Pamiętamy, że $\\theta = p$, więc zapis $L(X|p)$ jest równoważny $L(X|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcją likelihood dla tego rozkładu jest jego funkcja masy prawdopodobieństwa:\n",
    "\n",
    "$$f(X|\\theta) = f(X|p) = p^X(1-p)^{1-X}$$\n",
    "\n",
    "Stąd łączny likelihood to:\n",
    "\n",
    "$$L(X|\\theta) = L(X|p) = \\prod_{i=1}^n f(X_i) = \\prod_{i=1}^n p^{X_i}(1-p)^{1-X_i}$$\n",
    "\n",
    "Natomiast log likelihood to:\n",
    "\n",
    "$$LL(X|p) = \\sum_{i=1}^n log \\left(p^{X_i}(1-p)^{1-X_i}\\right)$$\n",
    "\n",
    "Korzystając w własności logarytmów otrzymujemy:\n",
    "\n",
    "$$LL(X|p) = \\sum_{i=1}^n \\left( X_i log(p) + (1 - X_i) log(1 - p) \\right) $$\n",
    "\n",
    "Aplikując operator sumy otrzymujemy:\n",
    "\n",
    "$$LL(X|p) = (\\sum_{i=1}^n X_i)log(p)  + (n - \\sum_{i=1}^n X_i)log(1-p)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby zmaksymalizować wartość log likelihood'u, a zatem również likelihood'u, wystarczy obliczyć pochodną tej funkcji względem parametru $p$ i przyrównać ją do zera:\n",
    "\n",
    "$$ \\frac{\\partial LL(X|p)}{\\partial p} = 0$$\n",
    "\n",
    "Obliczamy pochodną log likelihood'u:\n",
    "\n",
    "$$ \\frac{\\partial LL(X|p)}{\\partial p} = \\frac{\\partial}{\\partial p}\\left( (\\sum_{i=1}^n X_i)log(p)  + (n - \\sum_{i=1}^n X_i)log(1-p) \\right) = (\\sum_{i=1}^n X_i)\\frac{1}{p}  + (n - \\sum_{i=1}^n X_i)\\frac{-1}{1-p} $$\n",
    "\n",
    "Przyrównujemy wynik do zera:\n",
    "\n",
    "$$(\\sum_{i=1}^n X_i)\\frac{1}{p}  + (n - \\sum_{i=1}^n X_i)\\frac{-1}{1-p} = 0$$\n",
    "\n",
    "Oznaczmy $S = \\sum_{i=1}^n X_i$, wtedy:\n",
    "\n",
    "$$ \\frac{S}{p} = \\frac{n - S}{1-p}$$\n",
    "\n",
    "Przekształcając to równanie dalej:\n",
    "\n",
    "$$S(1-p) = p(n - S)$$\n",
    "$$S - Sp = np - Sp$$\n",
    "$$S = np$$\n",
    "\n",
    "Otrzmujemy zatem estymator parametru $p_\\text{MLE}$:\n",
    "\n",
    "$$\\theta_\\text{MLE} = p_\\text{MLE} = \\frac{\\sum_{i=1}^n X_i}{n}$$\n",
    "\n",
    "Widzimy zatem, że estymatorem MLE dla rozkładu Bernoulliego jest **średnia wartości próbek**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład rzutu monetą"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej rozważymy przykład wielokrotnego rzutu monetą i zbadamy jak się zachowuje estymator MLE.\n",
    "\n",
    "Zacznijmy od przygotowania danych. Wynikiem pojedynczego rzutu monetą jest jedno z dwóch możliwych zdarzeń: \"wypadnie orzeł\" (w danych oznaczony jako \"1\") lub \"wypadnie reszka\" (w danych - \"0\"). \n",
    "\n",
    "Rozkładem opisującym taki scenariusz jest właśnie rozkład Bernoulliego.\n",
    "\n",
    "Załóżmy, że rzeczywiste prawdopodobieństwo otrzymania orła jest równe $p = 0.7$ oraz, że eksperyment będziemy powtarzać $n = 1000$ razy.\n",
    "\n",
    "W tym ćwiczeniu będziemy używać rozkładów prawdopodobieństwa zaimplementowanych w bibliotece PyTorch (`pytorch.distributions`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(p: float, n: int) -> np.ndarray:\n",
    "    return ptdist.Bernoulli(probs=torch.tensor([p])).sample((n,)).squeeze(-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.7\n",
    "n = 1_000\n",
    "coin_data = bernoulli(p, n)\n",
    "\n",
    "print(coin_data.shape)\n",
    "print(coin_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystając z powyższych wyprowadzeń możemy zaimplementować estymator MLE dla rozkładu Bernoulliego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_bernoulli(x: np.ndarray) -> float:\n",
    "    n = len(x)\n",
    "    p_mle = sum(x) / n\n",
    "    \n",
    "    return p_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stosując go do naszego wygenerowanego zbioru danych otrzymujemy **wyestymowaną wartość parametru $p$ za pomocą metody MLE**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mle = mle_bernoulli(x=coin_data)\n",
    "\n",
    "display(Markdown(f\"### $p_{{MLE}}$ = {p_mle:.4f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy zatem, że otrzymana estymata wartości parametru rozkładu jest dość bliska rzeczywistej wartości parametru. Im więcej próbek byśmy mieli tym dokładniejsze byłoby przybliżenie, tzn. przy $n \\to \\infty$ wartość estymatora MLE zbiega do rzeczywistej wartości parametru $p$: \n",
    "\n",
    "$$\\lim_{n\\to\\infty} p_\\text{MLE} = p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Badanie zachowania estymatora MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początku sprawdźmy jak wartość estymatora $p_\\text{MLE}$ zależy **od liczby próbek**. Będziemy obliczać wartość $p_\\text{MLE}$ dla narastającej liczby próbek w zbiorze danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes, p_mle_values = evaluate_at_intervals(\n",
    "    data=coin_data[:200],  # Powyżej tej wartości estymator się nieznacznie zmienia    \n",
    "    step=2,\n",
    "    fn=lambda d, i: mle_bernoulli(x=d[:i]),\n",
    ")\n",
    "\n",
    "plot_param_values(\n",
    "    sample_sizes=sample_sizes,\n",
    "    param_values=p_mle_values, \n",
    "    true_param_value=p,\n",
    "    title=\"MLE dla rozkładu Bernoulliego\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy zatem, że po zobserwowaniu zaledwie kilkunastu/kilkudziesięciu próbek estymator $\\hat p$ zaczyna dobrze przybliżać wartość rzeczywistego parametru $p$. Należy jednak zauważyć, że początkowo wartości dość silnie się zmieniają. Zbadamy teraz jak zachowa się estymator $\\hat p$, jeśli **początkowo będziemy obserwować w zbiorze same reszki (\"0\") a dopiero później będą się pojawiać orły (\"1\").** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes, p_mle_values = evaluate_at_intervals(\n",
    "    data=sorted(coin_data[:200]),\n",
    "    step=2,\n",
    "    fn=lambda d, i: mle_bernoulli(x=d[:i]),\n",
    ")\n",
    "\n",
    "plot_param_values(\n",
    "    sample_sizes=sample_sizes,\n",
    "    param_values=p_mle_values, \n",
    "    true_param_value=p,\n",
    "    title=\"MLE dla rozkładu Bernoulliego (przy sortowaniu danych)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy estymator MLE jest **czuły na próbę losową niezgodną z rzeczywistym rozkładem danych**. Należy zatem pamiętać, że w przypadku stosowania estymatora MLE musimy dysponować odpowiednio dużą próbką danych. Często niestety koszty pozyskiwania danych są spore, ale możemy sobie poradzić z taką sytuacją w inny sposób -- **zastosować estymator Maximum a Posteriori (MAP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a Posteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowywując modele zazwyczaj mamy pewną **intuicję** na temat tego, jakich wyników się można spodziewać (*ang. apriori knowledge*). W kontekście uczenia Bayesowskiego taką intuicję będziemy nazywać **rozkładem apriori** (lub w skrócie **prior**) i oznaczać go jako $g(\\theta)$. Jest on określony dla wartości parametrów i wskazuje na to jak prawdopodobne są dane wartości parametrów modelu $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobierając rozkład apriori do zadanej funkcji likelihood należy pamiętać o **rozkładach sprzężonych**, które pozwalają na uproszczenie obliczeń w modelu. Poniżej przestawiono wybrane rozkłady sprzężone:\n",
    "\n",
    "| Likelihood                     | Conjugate Prior                   |\n",
    "| :----------------------------- | :-------------------------------- |\n",
    "| Bernoulli($p$)                 | Beta($\\alpha$,$\\beta$)            |\n",
    "| Binomial($p$)                  | Beta($\\alpha$,$\\beta$)            |\n",
    "| Poisson($\\lambda$)             | Gamma($\\alpha$,$\\beta$)           |\n",
    "| Categorical($\\mathbf{p}$, $k$) | Dirichlet($\\mathbf{\\alpha}$, $k$) |\n",
    "| Multinomial($\\mathbf{p}$, $k$) | Dirichlet($\\mathbf{\\alpha}$, $k$) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W podejściu Maximum a Posteriori chcemy wyznaczyć parametry modelu $\\theta_{\\text{MAP}}$, które maksymalizują funkcję **prawdopodobieństwa parametrów pod warunkiem obserwowanych danych** (nazywaną również **posterior**em): \n",
    "\n",
    "$$\\theta_\\text{MAP} = \\text{argmax}_{\\theta} f(\\theta | X_1, X_2, \\ldots, X_n)$$\n",
    "\n",
    "*Zauważ*: Estymator MLE stosuje \"odwrotną\" strategię, tzn. maksymalizuje prawdpodobieństwo obserwowania danych pod warunkiem określonych parametrów modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby rozwiązać tak zdefiniowany problem optymalizacyjny, spróbujmy najpierw zastosować tzw. **regułę Bayesa**:\n",
    "\n",
    "$$\\theta_\\text{MAP} = \\text{argmax}_\\theta \\frac{f(X_1, X_2, \\ldots, X_n | \\theta) g(\\theta)}{h(X_1, X_2, \\ldots, X_n)},$$\n",
    "\n",
    "gdzie $f(X_1, X_2, \\ldots, X_n | \\theta)$ to likelihood, $g(\\theta)$ to prior, a $h(X_1, X_2, \\ldots, X_n)$ reprezentuje prawdpodobieństwo obserwowania danych.\n",
    "\n",
    "Należy podkreślić, że funkcja $h(\\cdot)$ jest trudna w obliczeniu, ponieważ wymaga całkowania (sumowania) po całej dziedzinie możliwych wartości $\\theta$. Nie będzie to jednak stanowiło tutaj problemu, ponieważ nie interesuje nas konkretna wartość posteriora, tylko chcemy go zmaksymalizować. Nie potrzebujemy $h(\\cdot)$, a jako, że jest prawdopodobieństwem, to przyjmuje zawsze wartości nieujemne, więc możemy go pominąć w procesie maksymalizacji:\n",
    "\n",
    "$$\\theta_\\text{MAP} = \\text{argmax}_\\theta f(X_1, X_2, \\ldots, X_n | \\theta) g(\\theta)$$\n",
    "\n",
    "Przypominając sobie dodatkowo, że dane są IID otrzymujemy:\n",
    "\n",
    "$$\\theta_\\text{MAP} = \\text{argmax}_\\theta \\prod_{i=1}^n f(X_i | \\theta) g(\\theta)$$\n",
    "\n",
    "Podobnie jak w przypadku MLE możemy zastosować logarytm w celu uproszczenia obliczeń:\n",
    "\n",
    "$$\\theta_\\text{MAP} = \\text{argmax}_\\theta \\left( log(g(\\theta)) + \\sum_{i=1}^n log( f(X_i | \\theta)) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uwaga**: Zastosowanie MAP z rozkładem prior, który jest *non-informative* (np. rozkład jednostajny na przedziale $[-\\infty; +\\infty]$) **jest równoważne** estymacji Maximum Likelihood, gdy $n\\to\\infty$\n",
    "\n",
    "Zobacz również: [https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Example](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu wyznaczenia $\\theta_\\text{MAP}$ należy obliczyć pochodną powyższej funkcji (posterior; $\\text{Pt}$) ze względu na parametry $\\theta$ i przyrównać ją do zera:\n",
    "\n",
    "$$ \\text{Pt} = log(g(\\theta)) + \\sum_{i=1}^n log( f(X_i | \\theta)) $$\n",
    "\n",
    "$$ \\frac{\\partial \\text{Pt}}{\\partial \\theta} = 0$$\n",
    "\n",
    "\n",
    "Zatem otrzymujemy:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta}log(g(\\theta)) + \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^n log( f(X_i | \\theta)) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powrót do przykładu - rozkład Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przykładowym problemie wielokrotnego rzutu monetą założyliśmy, że likelihood może zostać zamodelowany za pomocą rozkładu Bernoulliego. Korzystając z tabeli rozkładów sprzężonych, widzimy że powinniśmy dobrać **rozkład Beta** jako prior.\n",
    "\n",
    "Rozkład Beta jest określony wzorem:\n",
    "\n",
    "$$\\text{Beta}(\\theta|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\theta^{\\alpha -1} (1 - \\theta)^{\\beta - 1},$$\n",
    "\n",
    "gdzie $\\alpha, \\beta$ to hiperparametry tego rozkładu a $\\Gamma(\\cdot)$ to funkcja Gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1 (1 pkt)\n",
    "W komórce poniżej oblicz pochodną logarytmu funkcji rozkładu Beta ze względu na $\\theta$:\n",
    "\n",
    "$$\\tag{1} \\frac{\\partial}{\\partial \\theta} log Beta(\\theta|\\alpha, \\beta) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "478b761ea8ce36b0c35de661f5710cde",
     "grade": true,
     "grade_id": "beta-derivative",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy okazji wyprowadzeń do MLE uzyskaliśmy już pochodną logarytmu likelihood'u (rozkład Bernoulliego):\n",
    "\n",
    "$$\\tag{2}\\frac{\\partial LL(X|\\theta)}{\\partial \\theta}  = \\frac{1}{\\theta} \\sum_{i=1}^n X_i - \\frac{1}{1-\\theta} (n - \\sum_{i=1}^n X_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2 (1 pkt)\n",
    "Podstaw wartości $(1)$ oraz $(2)$ do wzoru na estymację MAP i wyprowadź wzór na $\\theta_\\text{MAP}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff5fed8c482704ff631735b6cbb190d2",
     "grade": true,
     "grade_id": "map-solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3 (0.5 pkt)\n",
    "Zaimplementuj MAP zgodnie z wyprowadzonym wzorem. W poniższym kodzie przyjęto, że moneta jest (jak większość) symetryczna i jej zachowanie można opisać rozkładem $\\text{Beta}(2, 2)$. Obliczono również wartość estymatora MAP dla wygenerowanego zbioru danych `coin_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b51329113ff56bda1743d6fe7bf79c74",
     "grade": true,
     "grade_id": "map-implementation",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def beta_bernoulli_map(x: np.ndarray, alpha: float, beta: float) -> float:\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "\n",
    "alpha = 2\n",
    "beta = 2\n",
    "\n",
    "p_map = beta_bernoulli_map(\n",
    "    x=coin_data, \n",
    "    alpha=alpha, \n",
    "    beta=beta,\n",
    ")\n",
    "\n",
    "display(Markdown(f\"$p_{{MAP}} = {p_map:.4f}$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4 (0.5 pkt)\n",
    "Zbadaj zachowanie estymatora MAP dla różnych rozmiarów próbki danych. Wykorzystaj (i przerób) kod użyty w przykładzie z MLE. Zapisz obserwacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1dbf38905ddbda22735fedfcbce309f",
     "grade": true,
     "grade_id": "map-behavior-code",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obserwacje (wpisz w poniższej komórce):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c233bf6aaf895ab8e372439c7df17c19",
     "grade": true,
     "grade_id": "map-behavior-conclusions",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 5 (0.5 pkt)\n",
    "Zbadaj zachowanie estymatora MAP w przypadku obserowania na początku tylko samych \"0\" a dopiero później \"1\". Wykorzystaj (i przerób) kod z estymacji MLE. Powtórz eksperyment dla priora $\\text{Beta}(20, 20)$ oraz $\\text{Beta}(100, 100)$. Zapisz obserwacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e652a74de8e9cce81a398c7371cdb3f6",
     "grade": true,
     "grade_id": "map-behavior-different-prior",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obserwacje (wpisz w poniższej komórce):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "227e5664b5fe47fb93cf1d3b200ee0f4",
     "grade": true,
     "grade_id": "map-behavior-different-prior-conclusions",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przyrostowe uczenie (online learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli dane napływają w czasie rzeczywistym, to chcielibyśmy mieć sposób na aktualizację parametrów modelu na podstawie pojedynczych nowych próbek danych, ponieważ ponowne przeliczanie estymat parametrów \"od zera\" może być dla większych modeli dość czasochłonne. **Online learning** pozwala na takie podejście do aktualizacji wartości parametrów. Formalnie możemy to zapisać jako:\n",
    "\n",
    "$$\\tag{3} \\theta_\\text{MAP}^{(n+1)} = u(\\theta_\\text{MAP}^{(n)}, x_{n+1}),$$\n",
    "\n",
    "gdzie $\\theta_\\text{MAP}^{(n+1)}$ to zaktualizowane wartości parametrów ($n+1$ próbek), $\\theta_\\text{MAP}^{(n)}$ to poprzednie wartości parametrów ($n$ próbek), a $u(\\cdot, x_{n+1})$ to funkcja zaktualizująca wartości parametrów w oparciu o poprzednią wartość parametrów oraz nową próbkę danych.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 6 (1 pkt)\n",
    "Korzystając ze wzoru na MAP dla modelu Beta-Bernoulli oraz powyższego wzoru $(3)$ na uczenie przyrostowe, wyznacz wzór na $\\theta_\\text{MAP}^{(n+1)}$ w taki sposób, aby wykorzystać jak najwięcej obliczeń z poprzedniego kroku $\\theta_\\text{MAP}^{(n)}$. Rozpisz dokładną procedurę aktualizacji wartości parametrów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wzór na $\\theta_\\text{MAP}^{(n+1)}$ (wpisz w poniższej komórce):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "101ab30189c994fde3ebd050e7bc6c4a",
     "grade": true,
     "grade_id": "online-map-eq",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm aktualizacji parametrów (wpisz w poniższej komórce):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0135c2ce4e2ba0032a037ef545c867cc",
     "grade": true,
     "grade_id": "online-map-algorithm",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 7 (1 pkt)\n",
    "Zaimplementuj podejście przyrostowe oraz sprawdź, że daje ono takie same wyniki (lub bardzo zbliżone) jak w przypadku obliczania wartości estymatora MAP od zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a266e1e9d104018ce053ed3d2d1c354b",
     "grade": true,
     "grade_id": "online-map-implementation",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class OnlineMAP:\n",
    "    def __init__(self, alpha: int, beta: int):\n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _map(self):\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def update(self, x_i):\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dcca2b9c19886d10c2718c6e2b55ff6",
     "grade": true,
     "grade_id": "online-map-implementation-tests",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testy ukryte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f13de05d1365a747cef78fb9a941a580",
     "grade": true,
     "grade_id": "online-map-checks",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sample_sizes, p_map_full_values = evaluate_at_intervals(\n",
    "    data=coin_data[:50],\n",
    "    step=1,\n",
    "    fn=lambda d, i: beta_bernoulli_map(x=d[:i + 1], alpha=10, beta=10),\n",
    "    start=0,\n",
    ")\n",
    "\n",
    "plot_param_values(\n",
    "    sample_sizes=sample_sizes,\n",
    "    param_values=p_map_full_values, \n",
    "    true_param_value=p,\n",
    "    title=f\"MAP dla rozkładu Beta-Bernoulliego (pełne obliczenia)\",\n",
    ")\n",
    "\n",
    "online_map = OnlineMAP(alpha=10, beta=10)\n",
    "\n",
    "sample_sizes, p_map_online_values = None, None\n",
    "\n",
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()\n",
    "\n",
    "plot_param_values(\n",
    "    sample_sizes=sample_sizes,\n",
    "    param_values=p_map_online_values, \n",
    "    true_param_value=p,\n",
    "    title=f\"MAP dla rozkładu Beta-Bernoulliego (przyrostowe uczenie)\",\n",
    ")\n",
    "\n",
    "\n",
    "# Oblicz różnice wartości\n",
    "diff = None\n",
    "\n",
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Differences:', diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
